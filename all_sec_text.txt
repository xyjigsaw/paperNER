2 v 5 1 0 8 0 . 7 0 9 1 : v i X r a The evolution and development of events have their own basic principles, which make events happen sequentially. Therefore, the discovery of such evolutionary patterns among events are of great value for event prediction, decision-making and scenario design of dialog systems. However, conventional knowledge graph mainly focuses on the entities and their relations, which neglects the real world events. In this paper, we present a novel type of knowledge base ¡ª Event Logic Graph (ELG), which can reveal evolutionary patterns and development logics of real world events. Specifically, ELG is a directed cyclic graph, whose nodes are events, and edges stand for the sequential, causal, conditional or hypernym-hyponym (¡°is-a¡±) relations between events. We constructed two domain ELG: financial domain ELG, which consists of more than 1.5 million of event nodes and more than 1.8 million of directed edges, and travel domain ELG, which consists of about 30 thousand of event nodes and more than 234 thousand of directed edges. Experimental results show that ELG is effective for the task of script event prediction.
The evolution and development of events have their own underlying principles, leading to events happen sequentially. For example, the sentence ¡°After having lunch, Tom paid the bill and left the restaurant¡± shows a sequence of event evolutions: ¡°have lunch¡±!¡°pay the bill¡±!¡°leave the restaurant¡±. This event sequence is a common pattern for the scenario of having lunch in a restaurant. Such patterns can reveal the basic rules of event evolutions and human behaviors. Hence, it is of great value for many Artificial Intelligence (AI) applications, such as discourse understanding, intention identification and dialog generation.
However, traditional knowledge graph takes the entity as the research focus, and investigate the properties of entities and the relationships between entities, which lacks of event-related knowledge. In order to discover the evolutionary patterns and logics of events, we propose an eventcentric knowledge graph ¡ª Event Logic Graph (ELG) and the framework to construct ELG. ELG is a directed cyclic graph, whose nodes are events, and edges stand for the sequential (the same meaning with ¡°temporal¡±), causal, conditional or hypernym-hyponym (¡°is-a¡±) relations between events. Essentially, ELG is an event logic knowledge base, which can reveal evolutionary patterns and development logics of real world events.
To construct ELG, the first step is to extract events from raw texts, and then recognize the sequential, causal, conditional or hypernym-hyponym relations between two events and distinguish the direction of each sequential or causal relation. In the end, we need to merge event pairs to obtain the final event logic graph by connecting semantically similar events and generalizing each specific event.
Numerous efforts have been made to extract temporal and causal relations from texts. As the most commonly used corpus, TimeBank
The main contributions of this paper are threefold. First, we are among the first to propose the definition of ELG. Second, we propose a promising framework to construct ELG from a largescale unstructured text corpus. Third, experimental results show that ELG is capable of improving the performances of downstream applications, such as script event prediction. 2
Definition 1. ELG is a directed cyclic graph, whose nodes are events, and edges stand for the sequential, causal, conditional or hypernym-hyponym relations between events. Essentially, ELG is an event logic knowledge base, which reveals evolutionary patterns and development logics of real world events.
Figure 1 demonstrates three different event logic subgraphs of three different scenarios. Concretely, Figure 1 (a) describes a sequence of events under the scenario of ¡°plan a wedding¡±, which usually happen in the real world, and have evolved into the fixed human behavior patterns. For example, ¡°plan a wedding¡± usually follows by ¡°buy a house¡±, ¡°buy a car¡± and ¡°plan a travel¡±. This kind of commonsense event evolutionary patterns are usually hidden behind human beings¡¯ daily activities, or online user generated contents. To the best of our knowledge, this kind of commonsense knowledge is not explicitly stored in any existing knowledge bases, so that, we propose constructing an ELG to store it.
In ELG, events are represented as abstract, generalized and semantic complete event tuples E = (S; P; O), where P is the action, S is the actor and O is the object on which the action is performed. In our definition, each event must contain a trigger word (i.e., P ), such as ¡°run¡±, which mainly indicates the type of the event. In different application scenarios, S and O can be omitted, respectively, such as (watch, a movie) and (climate, warming). In general, events and the degree of abstraction of an event are closely related to the scene in which the event occurred, and a single event may become too abstract to understand without the context scenario.
Abstract and generalized means that we do not concern about the exact participants, location and time of an event. For example, ¡°who watch nounal hypernym¨Chyponym relation verbal hypernym¨Chyponym relation £¨food price, increase£© murder (vegetables price, increase) ¡­ (meat price, increase) homicide ¡­ suicide pri(cceu,ciunmcrbeearse) ¡­ (potato price, increase) (pork price, ¡­ (binecerfeparsiec)e, increase) assassinate ¡­ poisoning hang oneself ¡­ Take poison movies¡± and ¡°watch which movie¡± are not important in ELG. Semantic complete means that human beings can understand the meaning of the event without ambiguity. For example, (have, a hot pot) and (go to, the airport) are reasonable event tuples to represent events. While (go, somewhere), (do, the things) and (eat) are unreasonable or incomplete event representations, as their semantics are too vague to be understood.
We have four categories of directed edges: sequential directed edges, causal directed edges, conditional directed edges and hypernym-hyponym directed edges, which indicate different relationships between events. The sequential relation between two events refers to their partial temporal orderings. For example, given the sentence ¡°After having lunch, Tom paid the bill and left the restaurant.¡± (have, lunch), (pay, the bill) and (leave the restaurant) compose a sequential event chain.
The causal relation is the relation between the cause event and the effect event. For example, given the sentence ¡°The nuclear leak in Japan leads to serious ocean pollution.¡± (nuclear, leak) is the cause event, and (ocean, pollution) is the effect event. It is obvious that the causal relation must be sequential.
The conditional relation is a logical relation in which the illocutionary act employing one of a pair of propositions is expressed or implied to be true or in force if the other proposition is true. For example, ¡°study hard¡± is the condition of ¡°achieve good performances¡±.
The hypernym-hyponym relation is a kind of ¡°is-a¡± relationship between events, which includes two different types of hypernym-hyponym relations: nounal hypernym-hyponym relation and verbal hypernym-hyponym relation, as shown in Figure 2.
ELG is different from traditional Knowledge Graph in many aspects. As shown in Table 1, ELG focuses on events, and the directed edges between event nodes stand for the sequential, causal or hypernym-hyponym relations between them. The sequential and causal relations in ELG are probabilistic. In contrast, traditional Knowledge Graph focuses on entities, and its edges stand for the attributes of entities or relations between them. There are usually thousands of types of relations between entities. Moreover, the attributes and relations in Knowledge Graph are mostly deterministic. 3
As illustrated in Figure 3, we propose a framework to construct an ELG from large-scale unstructured texts, including data cleaning, natural language processing, event extraction, sequential relation and direction classification, causality extraction and transition probability computation.
After cleaning the data, a series of natural language processing steps including segmentation, part-of-speech tagging, and dependency parsing are conducted for event extraction. Tools provided Raw Corpus
Framework for Constructing ELG Data Cleaning
Event Extraction
NLP Processing by Language Technology Platform (LTP)
We extract structured events from free text using Open IE technology and dependency parsing. Given a sentence obtained from texts, we first adopt the event extraction methods described in
We filter out the low-frequency event tuples by a proper threshold, to exclude the event tuples extracted due to segmentation and dependency parsing errors. Some too general events such as ¡°go somewhere¡± and ¡°do something¡± are removed by regular expressions with a dictionary.
Given an event pair candidate (A, B), sequential relation recognition is to judge whether it has a sequential relation or not. For the ones having sequential relations, direction recognition should be conducted to distinguish the direction. For example, we need to recognize that there is a directed edge from (buy, tickets) to (watch, a movie). We regard the sequential relation and direction recognition as two separate binary classification tasks.
As shown in Table 2, multiple kinds of features are extracted for these two supervised classification tasks. Details about the intuition why we choose these features are described below:
T1: count of (A, B) R1: T2/T1 T2: count of (A, B) where A oc- R2: T1/T4 curs before B R3: T1/T5 T3: count of (A, B) where B oc- R4: T1/T6 curs before A R5: T1/T7 T4: count of A R6: T1/T8 T5: count of B R7: T1/T9 T6: count of verb-A R8: T6/T4 T7: count of object-A R9: T7/T4 T8: count of verb-B R10: T8/T5 T9: count of object-B R11: T9/T5
C1: the number of contexts in A1: PMI of verb-A which A and B co-occur and verb-B C2: average length of C1 A2: PMI of A and B C3: word embeddings of con- A3: PMI of verb-A texts in which A and B co-occur and object-B C4: the postag of contexts in A4: PMI of object-A which A and B co-occur and verb-B C5: phrase embeddings of A A5: PMI of object-A and B and object-B
event pair (A, B), the frequency-based features include their co-occur frequency (T1 to T3), respective frequency of A and B in the whole corpus (T4 and T5), and respective frequency of each event argument (T6 to T9).
binations between frequency-based features may provide extra information that is useful for sequential relation and direction classification, shown in Table 2 (R1 to R11).
contexts of event A and B are important features for identifying their sequential relation. We devise context-based features that capture the contextual semantic information of A and B, shown in Table 2 (C1 to C5).
PMI-based Features: We also investigate the pointwise mutual information (PMI) between event A and B, shown in Table 2 (A1 to A5).
event pair (A, B), we use the following equation to approximate the transition probability from the event A to the event B:
P (BjA) = f (A; B) f (A) ; (1) where f (A; B) is the co-occurrence frequency of event pair (A, B), and f (A) is the frequency of event A in the whole corpus.
O
B-cause CRF BiLSTM
C
T1
T2 E[CLS]
E1
E2 [CLS] Tok 1 Tok 2 £¨a£© £¨b£©
A A A A¡¯
B C B C ¡­ ¡­ ¡­ ¡­ ¡­ ¡­ ¡­ A A A¡¯ ¡­
B C B C
O TN EN Tok N 3.3.1
The first step to construct causal relation ELG is to identify cause-effect pairs from unstructured natural language texts. As the amount of data is extremely large (millions of documents), obtaining human-annotated pairs is impossible. We find that causal relations expressed in text have various forms. We therefore provide a procedure similar to our previous work
We construct a set of rules to extract mentions of causal events. Each rule follows the template of <Pattern, Constraint, Priority>, where Pattern is a regular expression containing a selected connector, Constraint is a syntactic constraint on sentences to which the pattern can be applied, and Priority is the priority of the rule if several rules are matched. For example, we use the pattern ¡°[cause] leads to [effect]¡± to extract the causal relation between two events. 3.3.2
As illustrated in Figure 4, we also use Bert and BiLSTM+CRF model to extract causal relations. Language model pre-training has shown to be very effective for learning universal language representations by leveraging large amounts of unlabeled data. Some of the most prominent models are ELMo
In this paper, we annotate each token in a sentence with following tags: B-cause, I-cause, Beffect, I-effect and O. The tag ¡°B-cause¡± refers to the beginning token of the cause event and each rest token in the cause event is represented by ¡°Icause¡±. The tag ¡°O¡± refers to the normal token which is irrelevant with causality. We feed the hidden representation Ti for each token i as the input layer of BiLSTM. These hidden representations Ti can be viewed as semantic features learnt from Bert model. The output representation layer of BiLSTM is then fed into the classification layer to predict the causal tags. The predictions in the classification layer are conditioned on the surrounding predictions by using the CRF method.
BERT Given large amount of event pairs extracted in previous steps, we need to connect event pairs to form a graph structure. Intuitively, as shown in Figure 5 (a), if we can find the same event in two event pairs, it is easy to form the graph structure. However, as the extracted events are discretely represented by bag-of-words, we can hardly find two identical events. Hence, as shown in Figure 5 (b), we propose to find the semantically similar events (A and A¡¯) and connect them. To this end, we propose learning distributed representations for each event, and utilize the cosine similarity to measure the semantic similarity between two event vectors. We use the framework of neural tensor networks to learn event embeddings, as described in our previous work
We investigate the application of the ELG on the task of script event prediction
Based on the ELG and our proposed scaled graph neural network (SGNN), we achieved the state-of-the-art performance of script event prediction in the work of
Moreover, we constructed a financial ELG, which consists of more than 1.5 million of event nodes and 1.8 million of directed edges. Figure 7 shows a screenshot of the online financial ELG (http://elg.8wss.com/). The user can enter any financial event in the search box, such as ¡°inflation¡±. Our demo can generate an event logic graph with ¡°inflation¡± as the initial node. The red node in Figure 7 is the input event; the yellow nodes are evolutionary nodes according to the input event, and the green nodes are semantically similar events with yellow nodes. We also give the extracted cause event, effect event and contexts in the right column to help users better understand the graph. Based on this financial ELG and the current events, we can infer what events will happen in the future. 4
We extract structured events from free text using Open IE technology and dependency parsing. Given a sentence obtained from texts, we first adopt the event extraction methods described in
We filter out the low-frequency event tuples by a proper threshold, to exclude the event tuples extracted due to segmentation and dependency parsing errors. Some too general events such as ¡°go somewhere¡± and ¡°do something¡± are removed by regular expressions with a dictionary.
Given an event pair candidate (A, B), sequential relation recognition is to judge whether it has a sequential relation or not. For the ones having sequential relations, direction recognition should be conducted to distinguish the direction. For example, we need to recognize that there is a directed edge from (buy, tickets) to (watch, a movie). We regard the sequential relation and direction recognition as two separate binary classification tasks.
As shown in Table 2, multiple kinds of features are extracted for these two supervised classification tasks. Details about the intuition why we choose these features are described below:
T1: count of (A, B) R1: T2/T1 T2: count of (A, B) where A oc- R2: T1/T4 curs before B R3: T1/T5 T3: count of (A, B) where B oc- R4: T1/T6 curs before A R5: T1/T7 T4: count of A R6: T1/T8 T5: count of B R7: T1/T9 T6: count of verb-A R8: T6/T4 T7: count of object-A R9: T7/T4 T8: count of verb-B R10: T8/T5 T9: count of object-B R11: T9/T5
C1: the number of contexts in A1: PMI of verb-A which A and B co-occur and verb-B C2: average length of C1 A2: PMI of A and B C3: word embeddings of con- A3: PMI of verb-A texts in which A and B co-occur and object-B C4: the postag of contexts in A4: PMI of object-A which A and B co-occur and verb-B C5: phrase embeddings of A A5: PMI of object-A and B and object-B
event pair (A, B), the frequency-based features include their co-occur frequency (T1 to T3), respective frequency of A and B in the whole corpus (T4 and T5), and respective frequency of each event argument (T6 to T9).
T1: count of (A, B) R1: T2/T1 T2: count of (A, B) where A oc- R2: T1/T4 curs before B R3: T1/T5 T3: count of (A, B) where B oc- R4: T1/T6 curs before A R5: T1/T7 T4: count of A R6: T1/T8 T5: count of B R7: T1/T9 T6: count of verb-A R8: T6/T4 T7: count of object-A R9: T7/T4 T8: count of verb-B R10: T8/T5 T9: count of object-B R11: T9/T5
C1: the number of contexts in A1: PMI of verb-A which A and B co-occur and verb-B C2: average length of C1 A2: PMI of A and B C3: word embeddings of con- A3: PMI of verb-A texts in which A and B co-occur and object-B C4: the postag of contexts in A4: PMI of object-A which A and B co-occur and verb-B C5: phrase embeddings of A A5: PMI of object-A and B and object-B
event pair (A, B), the frequency-based features include their co-occur frequency (T1 to T3), respective frequency of A and B in the whole corpus (T4 and T5), and respective frequency of each event argument (T6 to T9).
binations between frequency-based features may provide extra information that is useful for sequential relation and direction classification, shown in Table 2 (R1 to R11).
contexts of event A and B are important features for identifying their sequential relation. We devise context-based features that capture the contextual semantic information of A and B, shown in Table 2 (C1 to C5).
PMI-based Features: We also investigate the pointwise mutual information (PMI) between event A and B, shown in Table 2 (A1 to A5).
event pair (A, B), we use the following equation to approximate the transition probability from the event A to the event B:
P (BjA) = f (A; B) f (A) ; (1) where f (A; B) is the co-occurrence frequency of event pair (A, B), and f (A) is the frequency of event A in the whole corpus.
O
B-cause CRF BiLSTM
C
T1
T2 E[CLS]
E1
E2 [CLS] Tok 1 Tok 2 £¨a£© £¨b£©
A A A A¡¯
B C B C ¡­ ¡­ ¡­ ¡­ ¡­ ¡­ ¡­ A A A¡¯ ¡­
B C B C
O TN EN Tok N 3.3.1
The first step to construct causal relation ELG is to identify cause-effect pairs from unstructured natural language texts. As the amount of data is extremely large (millions of documents), obtaining human-annotated pairs is impossible. We find that causal relations expressed in text have various forms. We therefore provide a procedure similar to our previous work
We construct a set of rules to extract mentions of causal events. Each rule follows the template of <Pattern, Constraint, Priority>, where Pattern is a regular expression containing a selected connector, Constraint is a syntactic constraint on sentences to which the pattern can be applied, and Priority is the priority of the rule if several rules are matched. For example, we use the pattern ¡°[cause] leads to [effect]¡± to extract the causal relation between two events. 3.3.2
As illustrated in Figure 4, we also use Bert and BiLSTM+CRF model to extract causal relations. Language model pre-training has shown to be very effective for learning universal language representations by leveraging large amounts of unlabeled data. Some of the most prominent models are ELMo
In this paper, we annotate each token in a sentence with following tags: B-cause, I-cause, Beffect, I-effect and O. The tag ¡°B-cause¡± refers to the beginning token of the cause event and each rest token in the cause event is represented by ¡°Icause¡±. The tag ¡°O¡± refers to the normal token which is irrelevant with causality. We feed the hidden representation Ti for each token i as the input layer of BiLSTM. These hidden representations Ti can be viewed as semantic features learnt from Bert model. The output representation layer of BiLSTM is then fed into the classification layer to predict the causal tags. The predictions in the classification layer are conditioned on the surrounding predictions by using the CRF method.
BERT Given large amount of event pairs extracted in previous steps, we need to connect event pairs to form a graph structure. Intuitively, as shown in Figure 5 (a), if we can find the same event in two event pairs, it is easy to form the graph structure. However, as the extracted events are discretely represented by bag-of-words, we can hardly find two identical events. Hence, as shown in Figure 5 (b), we propose to find the semantically similar events (A and A¡¯) and connect them. To this end, we propose learning distributed representations for each event, and utilize the cosine similarity to measure the semantic similarity between two event vectors. We use the framework of neural tensor networks to learn event embeddings, as described in our previous work
We investigate the application of the ELG on the task of script event prediction
Based on the ELG and our proposed scaled graph neural network (SGNN), we achieved the state-of-the-art performance of script event prediction in the work of
Moreover, we constructed a financial ELG, which consists of more than 1.5 million of event nodes and 1.8 million of directed edges. Figure 7 shows a screenshot of the online financial ELG (http://elg.8wss.com/). The user can enter any financial event in the search box, such as ¡°inflation¡±. Our demo can generate an event logic graph with ¡°inflation¡± as the initial node. The red node in Figure 7 is the input event; the yellow nodes are evolutionary nodes according to the input event, and the green nodes are semantically similar events with yellow nodes. We also give the extracted cause event, effect event and contexts in the right column to help users better understand the graph. Based on this financial ELG and the current events, we can infer what events will happen in the future. 4
In this section, we conduct three kinds of experiments. First, we recognize whether two events has a sequential relation and its direction. Second, we extract casual relations between events based on our proposed unsupervised and supervised approaches. Finally, we use the downstream task: script event prediction to show the effectiveness of ELG. 4.1
We annotated 2,173 event pairs with high cooccurrence frequency ( 5) from the dataset. Each event pair (A, B) is ordered that A occurs before B with a higher frequency than B occurs before A. In the annotation process, the annotators are provided with the event pairs and their corresponding contexts. They need to judge whether there is a sequential relation between two events from a commonsense perspective. If true, they also need to give the sequential direction. For example, ¡°watch movies¡± and ¡°listen to music¡± are tagged as no sequential relation (negative), while ¡°go to the railway station¡± and ¡°by tickets¡± are tagged as having a sequential relation (positive), and the sequential direction is from the former to the latter (positive). The detailed statistics of our dataset are listed in Table 3. The positive and negative examples are very imbalanced. So we over sample the negative examples in training set to ensure the number of positive and negative training examples are equal.
For causal relation experiment, we crawled 1,362,345 Chinese financial news documents from online websites, such as Tencent1 and Netease2. All the headlines and main contents are exploited as the experiment corpus. We manually annotated 1,000 sentences to evaluate the causality extraction performance.
Script event prediction
For sequential relation recognition, PMI score of an event pair is used as the baseline method. For sequential direction recognition, if event A occurs before B with a higher frequency than B occurs before A, we regard the sequential direction as from event A to event B. This is called the Preceding Assumption, which is used as the baseline method for sequential direction recognition.
For sequential relation and direction recognition, four classifiers are used for these classification tasks, which are naive Bayes classifier (NB), logistic regression (LR), multiple layer perceptron (MLP) and support vector machines (SVM). We explored different feature combinations to find the best feature set for both classification tasks. All experiments are conducted using five-fold cross validation. The final experiment result is the average performance of ten times of implementations.
Two kinds of evaluation metrics are used to evaluate the performance of our proposed methods: accuracy and F1 metric.
1http://finance.qq.com/ 2http://money.163.com/ 4.2.2 For causal relation mining, we mainly conduct experiments to evaluate the causality extraction system. The same evaluation metrics as in sequential relation experiment are used to evaluate the performance of causality extraction. We mainly compare unsupervised rule-based causality extraction approach with supervised bert-based causality extraction approach. 4.2.3
We compare our model with the following baseline methods, and follow previous work
PMI
Bigram
Word2vec
DeepWalk
PairLSTM
Table 4 shows the experimental results for sequential relation classification, from which we find that the simple PMI baseline can achieve a good performance. Indeed, due to the imbalance of positive and negative test examples, PMI baseline chooses a threshold to classify all test examples as positive, and get a recall of 1. Four different classifiers using all the features in Table 2 achieve poor results, and only the NB classifier achieves higher performance than the baseline method. We explored all combinations of four kinds of features to find the best feature set for each classifier. The NB classifier achieves the best performance with the accuracy of 77.6% and the F1 score of 85.7% .
Table 5 shows the experimental results for sequential direction classification, from which we find that the Preceding Assumption is a very strong baseline for direction classification, and achieves an accuracy of 86.1% and a F1 score of 92.5%. Four classifiers with all features in Table 2 achieve poor results, and only the SVM achieves higher performance than the baseline method. We explored all combinations of four kinds of features, to find the best feature set for different classifiers. Still, the SVM classifier achieves the best performance with an accuracy of 87.0% and a F1 score of 92.9%, using the association and context based features.
Table 6 shows the experimental results for causal relation extraction, from which we find Bert-based approach dramatically outperforms rule-based approach. This is mainly because the BERT model can obtain general language knowledge from pretraining, and then our annotated data can be used to fine-tune the model to extract the causal relation. Rule-based approach can achieve better precision score but worse recall score, because manually constructed rules can hardly cover the whole linguistic phenomenons.
Experimental results are shown in Table 7, from which we can make the following observations.
(1) Word2vec, DeepWalk and other neural network-based models (EventComp, PairLSTM, SGNN) achieve significantly better results than the counting-based PMI and Bigram models. The main reason is that learning low dimensional dense embeddings for events is more effective than sparse feature representations for script event prediction.
(2) Comparison between ¡°Word2vec¡± and ¡°DeepWalk¡±, and between ¡°EventComp, PairLSTM¡± and ¡°SGNN¡± show that graph-based models achieve better performance than pair-based and chain-based models. This confirms our assumption that the event graph structure is more effective than event pairs and chains, and can provide more abundant event interactions information for script event prediction.
(3) Comparison between ¡°SGNN-attention¡± and ¡°SGNN¡± shows the attention mechanism can effectively improve the performance of SGNN. This indicates that different context events have different significance for choosing the correct subsequent event.
(4) SGNN achieves the best script event prediction performance of 52.45%, which is 3.2% improvement over the best baseline model (PairLSTM).
We also experimented with combinations of different models, to observe whether different models have complementary effects to each other. We find that SGNN+EventComp boosts the SGNN performance from 52.45% to 54.15%. This shows that they can benefit from each other. Nevertheless, SGNN+PairLSTM can only boost the SGNN performance from 52.45% to 52.71%. This is because the difference between SGNN and PairLSTM is not significant, which shows that they may learn similar event representations but SGNN learns in a better way. The combination of SGNN, EventComp and PairLSTM achieves the best performance of 54.93%. This is mainly because pair structure, chain structure and graph structure each has its own advantages and they can complement each other.
The learning curve (accuracy with time) of SGNN and PairLSTM is shown in Figure 8. We find that SGNN quickly reaches a stable high accuracy, and outperforms PairLSTM from start to the end. This demonstrates the advantages of SGNN over PairLSTM model. 5
We annotated 2,173 event pairs with high cooccurrence frequency ( 5) from the dataset. Each event pair (A, B) is ordered that A occurs before B with a higher frequency than B occurs before A. In the annotation process, the annotators are provided with the event pairs and their corresponding contexts. They need to judge whether there is a sequential relation between two events from a commonsense perspective. If true, they also need to give the sequential direction. For example, ¡°watch movies¡± and ¡°listen to music¡± are tagged as no sequential relation (negative), while ¡°go to the railway station¡± and ¡°by tickets¡± are tagged as having a sequential relation (positive), and the sequential direction is from the former to the latter (positive). The detailed statistics of our dataset are listed in Table 3. The positive and negative examples are very imbalanced. So we over sample the negative examples in training set to ensure the number of positive and negative training examples are equal.
For causal relation experiment, we crawled 1,362,345 Chinese financial news documents from online websites, such as Tencent1 and Netease2. All the headlines and main contents are exploited as the experiment corpus. We manually annotated 1,000 sentences to evaluate the causality extraction performance.
Script event prediction
For sequential relation recognition, PMI score of an event pair is used as the baseline method. For sequential direction recognition, if event A occurs before B with a higher frequency than B occurs before A, we regard the sequential direction as from event A to event B. This is called the Preceding Assumption, which is used as the baseline method for sequential direction recognition.
For sequential relation and direction recognition, four classifiers are used for these classification tasks, which are naive Bayes classifier (NB), logistic regression (LR), multiple layer perceptron (MLP) and support vector machines (SVM). We explored different feature combinations to find the best feature set for both classification tasks. All experiments are conducted using five-fold cross validation. The final experiment result is the average performance of ten times of implementations.
Two kinds of evaluation metrics are used to evaluate the performance of our proposed methods: accuracy and F1 metric.
1http://finance.qq.com/ 2http://money.163.com/ 4.2.2 For causal relation mining, we mainly conduct experiments to evaluate the causality extraction system. The same evaluation metrics as in sequential relation experiment are used to evaluate the performance of causality extraction. We mainly compare unsupervised rule-based causality extraction approach with supervised bert-based causality extraction approach. 4.2.3
We compare our model with the following baseline methods, and follow previous work
PMI
Bigram
Word2vec
DeepWalk
PairLSTM
Table 4 shows the experimental results for sequential relation classification, from which we find that the simple PMI baseline can achieve a good performance. Indeed, due to the imbalance of positive and negative test examples, PMI baseline chooses a threshold to classify all test examples as positive, and get a recall of 1. Four different classifiers using all the features in Table 2 achieve poor results, and only the NB classifier achieves higher performance than the baseline method. We explored all combinations of four kinds of features to find the best feature set for each classifier. The NB classifier achieves the best performance with the accuracy of 77.6% and the F1 score of 85.7% .
Table 5 shows the experimental results for sequential direction classification, from which we find that the Preceding Assumption is a very strong baseline for direction classification, and achieves an accuracy of 86.1% and a F1 score of 92.5%. Four classifiers with all features in Table 2 achieve poor results, and only the SVM achieves higher performance than the baseline method. We explored all combinations of four kinds of features, to find the best feature set for different classifiers. Still, the SVM classifier achieves the best performance with an accuracy of 87.0% and a F1 score of 92.9%, using the association and context based features.
Table 6 shows the experimental results for causal relation extraction, from which we find Bert-based approach dramatically outperforms rule-based approach. This is mainly because the BERT model can obtain general language knowledge from pretraining, and then our annotated data can be used to fine-tune the model to extract the causal relation. Rule-based approach can achieve better precision score but worse recall score, because manually constructed rules can hardly cover the whole linguistic phenomenons.
Experimental results are shown in Table 7, from which we can make the following observations.
(1) Word2vec, DeepWalk and other neural network-based models (EventComp, PairLSTM, SGNN) achieve significantly better results than the counting-based PMI and Bigram models. The main reason is that learning low dimensional dense embeddings for events is more effective than sparse feature representations for script event prediction.
(2) Comparison between ¡°Word2vec¡± and ¡°DeepWalk¡±, and between ¡°EventComp, PairLSTM¡± and ¡°SGNN¡± show that graph-based models achieve better performance than pair-based and chain-based models. This confirms our assumption that the event graph structure is more effective than event pairs and chains, and can provide more abundant event interactions information for script event prediction.
(3) Comparison between ¡°SGNN-attention¡± and ¡°SGNN¡± shows the attention mechanism can effectively improve the performance of SGNN. This indicates that different context events have different significance for choosing the correct subsequent event.
(4) SGNN achieves the best script event prediction performance of 52.45%, which is 3.2% improvement over the best baseline model (PairLSTM).
We also experimented with combinations of different models, to observe whether different models have complementary effects to each other. We find that SGNN+EventComp boosts the SGNN performance from 52.45% to 54.15%. This shows that they can benefit from each other. Nevertheless, SGNN+PairLSTM can only boost the SGNN performance from 52.45% to 52.71%. This is because the difference between SGNN and PairLSTM is not significant, which shows that they may learn similar event representations but SGNN learns in a better way. The combination of SGNN, EventComp and PairLSTM achieves the best performance of 54.93%. This is mainly because pair structure, chain structure and graph structure each has its own advantages and they can complement each other.
The learning curve (accuracy with time) of SGNN and PairLSTM is shown in Figure 8. We find that SGNN quickly reaches a stable high accuracy, and outperforms PairLSTM from start to the end. This demonstrates the advantages of SGNN over PairLSTM model. 5
The most relevant research area with ELG is script learning. The use of scripts in AI dates back to the 1970s
PairLSTM 800
Time (s) 200 400 600 itly model the temporal order of event pairs. However, they all utilized a very simple representation of event as the form of (verb, dependency). To overcome the drawback of this event representation, Pichotta and Mooney
There have been a number of recent neumodels for script learning. Pichotta and Mooney (2016) showed that LSTM-based event sequence model outperformed previous cooccurrence-based methods for event prediction.
Script learning is similar to ELG in concepts. However, script learning usually extracts event chains without considering their temporal orders and causal relations. ELG aims to organize event evolutionary patterns into a commonsense knowledge base, which is the biggest difference with script learning. 6
In this paper, we present an Event Logic Graph (ELG), which can reveal the evolutionary patterns and development logics of real world events. We also propose a framework to construct ELG from large-scale unstructured texts, and use the ELG to improve the performance of script event prediction. All techniques used in the ELG are languageindependent. Hence, we can easily construct other language versions of ELG.
This work is supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grants 61472107 and 61702137. The authors would like to thank the anonymous reviewers.
